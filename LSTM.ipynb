{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d7a0c4-3d00-4918-bb21-c6a65eb32bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lstm model...\n",
      "Epoch 1, Loss: 8.5105\n",
      "Epoch 2, Loss: 8.2992\n",
      "========================================\n",
      "Training bilstm model...\n",
      "Epoch 1, Loss: 8.5178\n",
      "Epoch 2, Loss: 8.1983\n",
      "========================================\n",
      "Training gru model...\n",
      "Epoch 1, Loss: 8.5276\n",
      "Epoch 2, Loss: 8.1397\n",
      "========================================\n",
      "Training bigru model...\n",
      "Epoch 1, Loss: 8.5319\n",
      "Epoch 2, Loss: 7.9329\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training transformer model...\n",
      "Epoch 1, Loss: 8.6476\n",
      "Epoch 2, Loss: 6.8612\n",
      "========================================\n",
      "Training 3dcnn_lstm model...\n",
      "Epoch 1, Loss: 8.5180\n",
      "Epoch 2, Loss: 8.3139\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --------------------------\n",
    "# CONFIGURATION\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = 5000\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "max_seq_len = 20\n",
    "batch_size = 4\n",
    "video_feat_dim = 1024  # Pretend video features vector size\n",
    "frames = 16  # Number of frames for 3D CNN\n",
    "height, width = 64, 64  # Frame spatial size for 3D CNN\n",
    "\n",
    "# --------------------------\n",
    "# MODELS\n",
    "# --------------------------\n",
    "\n",
    "class VideoCaptioningLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, embed_size)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, video_feat, captions):\n",
    "        video_feat = self.encoder(video_feat).unsqueeze(1)  # (B,1,embed)\n",
    "        embeddings = self.embed(captions)  # (B, seq_len, embed)\n",
    "        inputs = torch.cat((video_feat, embeddings), dim=1)  # prepend video feat\n",
    "        outputs, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, embed_size)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "\n",
    "    def forward(self, video_feat, captions):\n",
    "        video_feat = self.encoder(video_feat).unsqueeze(1)\n",
    "        embeddings = self.embed(captions)\n",
    "        inputs = torch.cat((video_feat, embeddings), dim=1)\n",
    "        outputs, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs\n",
    "\n",
    "class VideoCaptioningGRU(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, embed_size)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, video_feat, captions):\n",
    "        video_feat = self.encoder(video_feat).unsqueeze(1)\n",
    "        embeddings = self.embed(captions)\n",
    "        inputs = torch.cat((video_feat, embeddings), dim=1)\n",
    "        outputs, _ = self.gru(inputs)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs\n",
    "\n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, embed_size)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "\n",
    "    def forward(self, video_feat, captions):\n",
    "        video_feat = self.encoder(video_feat).unsqueeze(1)\n",
    "        embeddings = self.embed(captions)\n",
    "        inputs = torch.cat((video_feat, embeddings), dim=1)\n",
    "        outputs, _ = self.gru(inputs)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs\n",
    "\n",
    "class TransformerCaptioning(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size, vocab_size, num_heads=4, num_layers=2, max_seq_len=20):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, embed_size)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, max_seq_len + 1, embed_size))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, video_feat, captions):\n",
    "        B = captions.size(0)\n",
    "        video_feat = self.encoder(video_feat).unsqueeze(1)\n",
    "        embeddings = self.embed(captions)\n",
    "        x = torch.cat([video_feat, embeddings], dim=1)\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        x = self.transformer(x)\n",
    "        outputs = self.fc(x)\n",
    "        return outputs\n",
    "\n",
    "class CNN3D_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.cnn3d = nn.Sequential(\n",
    "            nn.Conv3d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(2),\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool3d((1,1,1))\n",
    "        )\n",
    "        self.fc_feat = nn.Linear(128, embed_size)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, video_clip, captions):\n",
    "        B = video_clip.size(0)\n",
    "        features = self.cnn3d(video_clip).view(B, -1)  # (B, 128)\n",
    "        video_emb = self.fc_feat(features).unsqueeze(1)  # (B, 1, embed)\n",
    "        embedded = self.embed(captions)  # (B, seq_len, embed)\n",
    "        inputs = torch.cat((video_emb, embedded), dim=1)\n",
    "        outputs, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs\n",
    "\n",
    "# --------------------------\n",
    "# HELPER: SELECT MODEL\n",
    "# --------------------------\n",
    "def get_model(model_name):\n",
    "    if model_name == 'lstm':\n",
    "        return VideoCaptioningLSTM(video_feat_dim, embed_size, hidden_size, vocab_size)\n",
    "    elif model_name == 'bilstm':\n",
    "        return BiLSTM(video_feat_dim, embed_size, hidden_size, vocab_size)\n",
    "    elif model_name == 'gru':\n",
    "        return VideoCaptioningGRU(video_feat_dim, embed_size, hidden_size, vocab_size)\n",
    "    elif model_name == 'bigru':\n",
    "        return BiGRU(video_feat_dim, embed_size, hidden_size, vocab_size)\n",
    "    elif model_name == 'transformer':\n",
    "        return TransformerCaptioning(video_feat_dim, embed_size, vocab_size, max_seq_len=max_seq_len)\n",
    "    elif model_name == '3dcnn_lstm':\n",
    "        return CNN3D_LSTM(vocab_size, embed_size, hidden_size)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "# --------------------------\n",
    "# DUMMY DATA FOR TESTING\n",
    "# --------------------------\n",
    "# For non-3dcnn models: video features + captions\n",
    "video_feats = torch.randn(batch_size, video_feat_dim).to(device)\n",
    "captions = torch.randint(0, vocab_size, (batch_size, max_seq_len)).to(device)\n",
    "\n",
    "# For 3D CNN model: raw video clip + captions\n",
    "video_clips = torch.randn(batch_size, 3, frames, height, width).to(device)\n",
    "\n",
    "# --------------------------\n",
    "# TRAIN LOOP EXAMPLE\n",
    "# --------------------------\n",
    "def train(model_name='lstm', epochs=2):\n",
    "    model = get_model(model_name).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    print(f\"Training {model_name} model...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if model_name == '3dcnn_lstm':\n",
    "            outputs = model(video_clips, captions)\n",
    "        else:\n",
    "            outputs = model(video_feats, captions)\n",
    "\n",
    "        # outputs shape: (batch, seq_len+1, vocab_size)\n",
    "        # We want to predict next words, so shift captions by one for target\n",
    "        targets = captions  # Simplified target\n",
    "        outputs = outputs[:, 1:, :]  # remove first token prediction to match targets length\n",
    "\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# RUN TRAINING FOR ALL MODELS\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    for model_name in ['lstm', 'bilstm', 'gru', 'bigru', 'transformer', '3dcnn_lstm']:\n",
    "        train(model_name)\n",
    "        print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d42a6-f7b7-4bdb-b0c5-88f9129c4359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
